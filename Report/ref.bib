@inproceedings{textattack,   title={TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP},  url={http://dx.doi.org/10.18653/v1/2020.emnlp-demos.16},  DOI={10.18653/v1/2020.emnlp-demos.16},  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},  author={Morris, John and Lifland, Eli and Yoo, Jin Yong and Grigsby, Jake and Jin, Di and Qi, Yanjun},  year={2020},  month={Jan},  language={en-US}  }

@article{gptfuzz,   title={GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts},  author={Yu, Jiahao and Lin, Xingwei},  language={en-US}  }

@article{renellm,   title={A Wolf in Sheepâ€™s Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily},  author={Ding, Peng and Kuang, Jun and Ma, Dan and Cao, Xuezhi and Xian, Yunsen and Chen, Jiajun and Huang, Shujian},  year={2023},  month={Nov},  language={en-US}  }

@article{tap,   title={Tree of Attacks: Jailbreaking Black-Box LLMs Automatically},  author={Mehrotra, Anay and Zampetakis, Manolis and Kassianik, Paul and Nelson, Blaine and Anderson, Hyrum and Singer, Yaron and Karbasi, Amin},  year={2023},  month={Dec},  language={en-US}  }

@article{jailbroken,   title={Jailbroken: How Does LLM Safety Training Fail?},  author={Wei, Alexander and Haghtalab, Nika and Steinhardt, Jacob},  year={2023},  month={Jul},  language={en-US}  }


@article{multilingual,
  title={Multilingual jailbreak challenges in large language models},
  author={Deng, Yue and Zhang, Wenxuan and Pan, Sinno Jialin and Bing, Lidong},
  journal={arXiv preprint arXiv:2310.06474},
  year={2023}
}

@article{DeepInception,   title={DeepInception: Hypnotize Large Language Model to Be Jailbreaker},  author={Li, Xuan and Zhou, Zhanke and Zhu, Jianing and Yao, Jiangchao and Liu, Tongliang and Han, Bo},  year={2023},  month={Nov},  language={en-US}  }

@article{pair,   title={Jailbreaking Black Box Large Language Models in Twenty Queries},  author={Chao, Patrick and Robey, Alexander and Dobriban, Edgar and Hassani, Hamed and Pappas, GeorgeJ. and Wong, Eric},  year={2023},  month={Oct},  language={en-US}  }

@article{ica,   title={Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations},  author={Wei, Zeming and Wang, Yifei and Wang, Yisen},  language={en-US}  }

@article{gcg,   title={Universal and Transferable Adversarial Attacks on Aligned Language Models},  author={Zou, Andy and Wang, Zifan and Kolter, J.Zico and Fredrikson, Matt},  year={2023},  month={Jul},  language={en-US}  }

@article{jailbreak_survey,   title={Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study},  author={Liu, Yi and Deng, Gelei and Xu, Zhengzi and Li, Yuekang and Zheng, Yaowen and Zhang, Ying and Zhao, Lida and Zhang, Tianwei and Liu, Yang},  language={en-US}  }

@article{autodanliu2023,
  title={Autodan: Generating stealthy jailbreak prompts on aligned large language models},
  author={Liu, Xiaogeng and Xu, Nan and Chen, Muhao and Xiao, Chaowei},
  journal={arXiv preprint arXiv:2310.04451},
  year={2023}
}

@article{cipher,
  title={Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher},
  author={Yuan, Youliang and Jiao, Wenxiang and Wang, Wenxuan and Huang, Jen-tse and He, Pinjia and Shi, Shuming and Tu, Zhaopeng},
  journal={arXiv preprint arXiv:2308.06463},
  year={2023}
}

@article{bai2023qwen,
  title={Qwen technical report},
  author={Bai, Jinze and Bai, Shuai and Chu, Yunfei and Cui, Zeyu and Dang, Kai and Deng, Xiaodong and Fan, Yang and Ge, Wenbin and Han, Yu and Huang, Fei and others},
  journal={arXiv preprint arXiv:2309.16609},
  year={2023}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{vicuna,
  title={Judging LLM-as-a-judge with MT-Bench and Chatbot Arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
  journal={arXiv preprint arXiv:2306.05685},
  year={2023}
}

@inproceedings{du2022glm,
  title={GLM: General Language Model Pretraining with Autoregressive Blank Infilling},
  author={Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={320--335},
  year={2022}
}

@misc{2023internlm,
    title={InternLM: A Multilingual Language Model with Progressively Enhanced Capabilities},
    author={InternLM Team},
    howpublished = {\url{https://github.com/InternLM/InternLM}},
    year={2023}
}

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@article{lv2024codechameleon,
  title={CodeChameleon: Personalized Encryption Framework for Jailbreaking Large Language Models},
  author={Lv, Huijie and Wang, Xiao and Zhang, Yuansen and Huang, Caishuang and Dou, Shihan and Ye, Junjie and Gui, Tao and Zhang, Qi and Huang, Xuanjing},
  journal={arXiv preprint arXiv:2402.16717},
  year={2024}
}

@inproceedings{shayegani2023jailbreak,
  title={Jailbreak in pieces: Compositional adversarial attacks on multi-modal language models},
  author={Shayegani, Erfan and Dong, Yue and Abu-Ghazaleh, Nael},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

@article{li2023multi,
  title={Multi-step jailbreaking privacy attacks on chatgpt},
  author={Li, Haoran and Guo, Dadi and Fan, Wei and Xu, Mingshi and Song, Yangqiu},
  journal={arXiv preprint arXiv:2304.05197},
  year={2023}
}

@article{huang2023catastrophic,
  title={Catastrophic jailbreak of open-source llms via exploiting generation},
  author={Huang, Yangsibo and Gupta, Samyak and Xia, Mengzhou and Li, Kai and Chen, Danqi},
  journal={arXiv preprint arXiv:2310.06987},
  year={2023}
}
@article{zeng2024johnny,
  title={How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms},
  author={Zeng, Yi and Lin, Hongpeng and Zhang, Jingwen and Yang, Diyi and Jia, Ruoxi and Shi, Weiyan},
  journal={arXiv preprint arXiv:2401.06373},
  year={2024}
}

@article{yao2023fuzzllm,
  title={Fuzzllm: A novel and universal fuzzing framework for proactively discovering jailbreak vulnerabilities in large language models},
  author={Yao, Dongyu and Zhang, Jianshu and Harris, Ian G and Carlsson, Marcel},
  journal={arXiv preprint arXiv:2309.05274},
  year={2023}
}

@article{deng2023jailbreaker,
  title={Jailbreaker: Automated jailbreak across multiple large language model chatbots},
  author={Deng, Gelei and Liu, Yi and Li, Yuekang and Wang, Kailong and Zhang, Ying and Li, Zefeng and Wang, Haoyu and Zhang, Tianwei and Liu, Yang},
  journal={arXiv preprint arXiv:2307.08715},
  year={2023}
}

@article{Shen2023DoAN,
  title={"Do Anything Now": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models},
  author={Xinyue Shen and Zeyuan Johnson Chen and Michael Backes and Yun Shen and Yang Zhang},
  journal={ArXiv},
  year={2023},
  volume={abs/2308.03825},
  url={https://api.semanticscholar.org/CorpusID:260704242}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@inproceedings{Sadasivan2024FastAA,
  title={Fast Adversarial Attacks on Language Models In One GPU Minute},
  author={Vinu Sankar Sadasivan and Shoumik Saha and Gaurang Sriramanan and Priyatham Kattakinda and Atoosa Malemir Chegini and Soheil Feizi},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:267938703}
}

@article{Lapid2023OpenSU,
  title={Open Sesame! Universal Black Box Jailbreaking of Large Language Models},
  author={Raz Lapid and Ron Langberg and Moshe Sipper},
  journal={ArXiv},
  year={2023},
  volume={abs/2309.01446},
  url={https://api.semanticscholar.org/CorpusID:261530019}
}

@article{Helbling2023LLMSD,
  title={LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked},
  author={Alec Helbling and Mansi Phute and Matthew Hull and Duen Horng Chau},
  journal={ArXiv},
  year={2023},
  volume={abs/2308.07308},
  url={https://api.semanticscholar.org/CorpusID:260887487}
}

@article{Jain2023BaselineDF,
  title={Baseline Defenses for Adversarial Attacks Against Aligned Language Models},
  author={Neel Jain and Avi Schwarzschild and Yuxin Wen and Gowthami Somepalli and John Kirchenbauer and Ping-yeh Chiang and Micah Goldblum and Aniruddha Saha and Jonas Geiping and Tom Goldstein},
  journal={ArXiv},
  year={2023},
  volume={abs/2309.00614},
  url={https://api.semanticscholar.org/CorpusID:261494182}
}

@article{Robey2023SmoothLLMDL,
  title={SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks},
  author={Alexander Robey and Eric Wong and Hamed Hassani and George J Pappas},
  journal={ArXiv},
  year={2023},
  volume={abs/2310.03684},
  url={https://api.semanticscholar.org/CorpusID:263671542}
}

@article{Cao2023DefendingAA,
  title={Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM},
  author={Bochuan Cao and Yu Cao and Lu Lin and Jinghui Chen},
  journal={ArXiv},
  year={2023},
  volume={abs/2309.14348},
  url={https://api.semanticscholar.org/CorpusID:262827619}
}

@article{yang2023shadow,
  title={Shadow alignment: The ease of subverting safely-aligned language models},
  author={Yang, Xianjun and Wang, Xiao and Zhang, Qi and Petzold, Linda and Wang, William Yang and Zhao, Xun and Lin, Dahua},
  journal={arXiv preprint arXiv:2310.02949},
  year={2023}
}